{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from rich import print as pp\n",
    "from icecream import ic\n",
    "import gc\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rich.progress import track\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0758d727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967da117",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/betl2train_with_s_a_id.json', 'r') as file_reader:\n",
    "    betl2train = json.load(file_reader)\n",
    "\n",
    "with open('../hf_token.txt', 'r') as file_reader:\n",
    "    hf_token = file_reader.readlines()[0]\n",
    "\n",
    "base_df = pd.read_csv('./results/0_base_data_no_result.csv')\n",
    "\n",
    "# pp(hf_token)\n",
    "\n",
    "model_names = ['microsoft/Phi-4-mini-instruct',\n",
    "               'meta-llama/Llama-3.2-3B-Instruct',\n",
    "               'google/gemma-3-4b-it',\n",
    "               'Qwen/Qwen3-4B']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# col_names = ['s_a_id', 'q_id', 'label'] + model_names\n",
    "\n",
    "# df = pd.DataFrame(columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c7d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47f304c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in base_df:\n",
    "#     base_df[i] = base_df[i].astype(str)\n",
    "#     pp(base_df[i].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6808ce43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d971b556edae477c8d9c31b8ea4b8e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a9cb3182cf4bab8313b4d953e70bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| torch.cuda.memory_allocated(): 7680568320\n",
      "ic| torch.cuda.memory_allocated(): 8524288\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "\n",
    "    # ic(model_name)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        device_map = device, \n",
    "        torch_dtype = \"auto\", \n",
    "        trust_remote_code = True,\n",
    "        token = hf_token\n",
    "    )\n",
    "\n",
    "    # ic(model)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # ic(tokenizer)\n",
    "    m_pred = []\n",
    "    # idx = 1\n",
    "\n",
    "    # for s_a_id in betl2train:\n",
    "    for s_a_id, _ in track(betl2train.items(), description = \"Processing samples\"):\n",
    "\n",
    "        prompt = f'''You are an expert grader. You will be provided with a question, a set of reference answers which should be considered the golden standard, and a student's answer. Your task is to grade the student's answer as 'correct' or  'incorrect' based *only* on its semantic alignment with the reference answer. Output *only* the single word grade ('correct' or 'incorrect') and nothing else.\\n\\nQuestion: {betl2train[s_a_id]['q']}\\n'''\n",
    "\n",
    "        r_a = \"\"\n",
    "        for i in range(len(betl2train[s_a_id]['r_a'])):\n",
    "            r_a = r_a + f'''\\nReference Answer {i + 1}: {betl2train[s_a_id]['r_a'][i]}\\n'''\n",
    "        \n",
    "        prompt = prompt + r_a + f'''\\nStudent Answer: {betl2train[s_a_id]['s_a']}\\n\\nGrade: '''\n",
    "\n",
    "        # ic(prompt)\n",
    "\n",
    "        text_tokenized = tokenizer(prompt, return_tensors = 'pt').to(device)\n",
    "        tokens_to_gen = 2\n",
    "\n",
    "        output = model.generate(\n",
    "            text_tokenized['input_ids'],\n",
    "            do_sample = True,\n",
    "            top_p = 0.95,\n",
    "            temperature = 0.001,\n",
    "            top_k = 0,\n",
    "            max_new_tokens = tokens_to_gen)\n",
    "        \n",
    "        model_op = tokenizer.decode(output[0])\n",
    "        model_pred = model_op[len(prompt):].strip().replace('<|endoftext|>', '').lower()\n",
    "        # ic(model_op)\n",
    "        # ic(model_pred)\n",
    "        m_pred.append(model_pred)\n",
    "        # base_df.loc[str(int(s_a_id) + 1), model_name] = model_pred\n",
    "\n",
    "        # ic(idx)\n",
    "        # idx += 1\n",
    "        # if idx == 20:\n",
    "        #     break\n",
    "    \n",
    "    base_df[model_name] = m_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ic(torch.cuda.memory_allocated())\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    ic(torch.cuda.memory_allocated())\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565be08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_a_id</th>\n",
       "      <th>q_id</th>\n",
       "      <th>label</th>\n",
       "      <th>microsoft/Phi-4-mini-instruct</th>\n",
       "      <th>google/gemma-3-4b-it</th>\n",
       "      <th>meta-llama/Llama-3.2-3B-Instruct</th>\n",
       "      <th>Qwen/Qwen3-4B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>correct</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>correct</td>\n",
       "      <td>correct</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>correct</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3936</th>\n",
       "      <td>3937</td>\n",
       "      <td>47</td>\n",
       "      <td>correct</td>\n",
       "      <td>correct</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3937</th>\n",
       "      <td>3938</td>\n",
       "      <td>47</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>correct</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3938</th>\n",
       "      <td>3939</td>\n",
       "      <td>47</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>correct</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3939</th>\n",
       "      <td>3940</td>\n",
       "      <td>47</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3940</th>\n",
       "      <td>3941</td>\n",
       "      <td>47</td>\n",
       "      <td>correct</td>\n",
       "      <td>correct</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3941 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      s_a_id  q_id      label microsoft/Phi-4-mini-instruct  \\\n",
       "0          1     1    correct                     incorrect   \n",
       "1          2     1    correct                       correct   \n",
       "2          3     1  incorrect                     incorrect   \n",
       "3          4     1  incorrect                       correct   \n",
       "4          5     1  incorrect                     incorrect   \n",
       "...      ...   ...        ...                           ...   \n",
       "3936    3937    47    correct                       correct   \n",
       "3937    3938    47  incorrect                       correct   \n",
       "3938    3939    47  incorrect                       correct   \n",
       "3939    3940    47  incorrect                     incorrect   \n",
       "3940    3941    47    correct                       correct   \n",
       "\n",
       "      google/gemma-3-4b-it  meta-llama/Llama-3.2-3B-Instruct  Qwen/Qwen3-4B  \n",
       "0                      NaN                               NaN            NaN  \n",
       "1                      NaN                               NaN            NaN  \n",
       "2                      NaN                               NaN            NaN  \n",
       "3                      NaN                               NaN            NaN  \n",
       "4                      NaN                               NaN            NaN  \n",
       "...                    ...                               ...            ...  \n",
       "3936                   NaN                               NaN            NaN  \n",
       "3937                   NaN                               NaN            NaN  \n",
       "3938                   NaN                               NaN            NaN  \n",
       "3939                   NaN                               NaN            NaN  \n",
       "3940                   NaN                               NaN            NaN  \n",
       "\n",
       "[3941 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9462b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'tp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fn'</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'fp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tn'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'tp'\u001b[0m, \u001b[32m'fn'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m[\u001b[0m\u001b[32m'fp'\u001b[0m, \u001b[32m'tn'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7785b4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'tp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fn'</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'fp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tn'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'tp'\u001b[0m, \u001b[32m'fn'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m[\u001b[0m\u001b[32m'fp'\u001b[0m, \u001b[32m'tn'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1247</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">418</span><span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">682</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1594</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1247\u001b[0m  \u001b[1;36m418\u001b[0m\u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m \u001b[1;36m682\u001b[0m \u001b[1;36m1594\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual_values = base_df['label']\n",
    "predicted_values = base_df['microsoft/Phi-4-mini-instruct']\n",
    "conf_matrix = confusion_matrix(actual_values, predicted_values)\n",
    "x = [['tp', 'fn'],['fp', 'tn']]\n",
    "pp(*x, sep = '\\n')\n",
    "pp(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd97329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76abbdf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'tp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fn'</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'fp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tn'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'tp'\u001b[0m, \u001b[32m'fn'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m[\u001b[0m\u001b[32m'fp'\u001b[0m, \u001b[32m'tn'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>microsoft/Phi-4-mini-instruct</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>correct</th>\n",
       "      <td>1247</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incorrect</th>\n",
       "      <td>682</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "microsoft/Phi-4-mini-instruct  correct  incorrect\n",
       "label                                            \n",
       "correct                           1247        418\n",
       "incorrect                          682       1594"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [['tp', 'fn'],['fp', 'tn']]\n",
    "pp(*x, sep = '\\n')\n",
    "base_df.groupby(['label', 'microsoft/Phi-4-mini-instruct']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e8a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97cbf3ac82d4803b1d306d9fce583ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf70c90550434705806649e9979e7f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rich.progress import track\n",
    "import torch\n",
    "\n",
    "torch.random.manual_seed(666)\n",
    "\n",
    "with open('./data/betl2train_with_s_a_id.json', 'r') as file_reader:\n",
    "    betl2train = json.load(file_reader)\n",
    "\n",
    "model_op = dict()\n",
    "\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "model_op[model_name] = []\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype = \"auto\",\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for s_a_id, _ in track(betl2train.items(), description = \"Processing samples\"):\n",
    "\n",
    "    prompt = f'''Question: {betl2train[s_a_id]['q']}\\n'''\n",
    "\n",
    "    r_a = \"\"\n",
    "    for i in range(len(betl2train[s_a_id]['r_a'])):\n",
    "        r_a = r_a + f'''\\nReference Answer {i + 1}: {betl2train[s_a_id]['r_a'][i]}\\n'''\n",
    "\n",
    "    prompt = prompt + r_a + f'''\\nStudent Answer: {betl2train[s_a_id]['s_a']}\\n\\nWhat is the Student's grade?'''\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert grader. You will be provided with a question, a set of reference answers which should be considered the golden standard, and a student's answer. Your task is to grade the student's answer as 'correct' or  'incorrect' based *only* on its semantic alignment with the reference answer. Output *EXACTLY ONLY ONLY ONE SINGLE WORD GRADE* that is either 'correct' or 'incorrect' and *NOTHING ELSE*.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True,\n",
    "        enable_thinking = False\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "    try:\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    model_op[model_name].append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93817def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3941\n"
     ]
    }
   ],
   "source": [
    "print(len(model_op[model_name]))\n",
    "\n",
    "with open('./results/t0_qwen_no_thought_results.txt', 'w') as f:\n",
    "    for line in model_op[model_name]:\n",
    "        f.write(f\"{str(line).strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe99d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7ad664fb464f8eaf2a9679f84bd247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'correct'\u001b[0m,\n",
       "    \u001b[32m'correct'\u001b[0m,\n",
       "    \u001b[32m'incorrect'\u001b[0m,\n",
       "    \u001b[32m'correct'\u001b[0m,\n",
       "    \u001b[32m'incorrect'\u001b[0m,\n",
       "    \u001b[32m'incorrect'\u001b[0m,\n",
       "    \u001b[32m'incorrect'\u001b[0m,\n",
       "    \u001b[32m'correct'\u001b[0m,\n",
       "    \u001b[32m'correct'\u001b[0m,\n",
       "    \u001b[32m'incorrect'\u001b[0m,\n",
       "    \u001b[32m'incorrect'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from rich import print as pp\n",
    "\n",
    "torch.random.manual_seed(666)\n",
    "\n",
    "with open('./data/betl2train_with_s_a_id.json', 'r') as file_reader:\n",
    "    betl2train = json.load(file_reader)\n",
    "\n",
    "model_op = dict()\n",
    "\n",
    "# model_name = \"google/gemma-3-4b-it\"\n",
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model_name = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "model_op[model_name] = []\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model_name,\n",
    "    device = \"cuda\",\n",
    "    torch_dtype = torch.bfloat16,\n",
    ")\n",
    "\n",
    "m = []\n",
    "\n",
    "\n",
    "for cnt, s_a_id in enumerate(betl2train):\n",
    "\n",
    "    prompt = f'''Question: {betl2train[s_a_id]['q']}\\n'''\n",
    "\n",
    "    r_a = \"\"\n",
    "    for i in range(len(betl2train[s_a_id]['r_a'])):\n",
    "        r_a = r_a + f'''\\nReference Answer {i + 1}: {betl2train[s_a_id]['r_a'][i]}\\n'''\n",
    "\n",
    "    prompt = prompt + r_a + f'''\\nStudent Answer: {betl2train[s_a_id]['s_a']}\\n\\nWhat is the Student's grade?'''\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert grader. You will be provided with a question, a set of reference answers which should be considered the golden standard, and a student's answer. Your task is to grade the student's answer as 'correct' or  'incorrect' based *only* on its semantic alignment with the reference answer. Output *EXACTLY ONLY ONLY ONE SINGLE WORD GRADE* that is either 'correct' or 'incorrect' and *NOTHING ELSE*.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    m.append(messages)\n",
    "\n",
    "    if cnt == 10:\n",
    "        break\n",
    "\n",
    "output = pipe(m,\n",
    "              do_sample = True,\n",
    "              max_new_tokens = 2,\n",
    "              return_full_text = False,\n",
    "              temperature =  0.1,)\n",
    "            #   batch_size = 64)\n",
    "\n",
    "\n",
    "# pp([op[0][\"generated_text\"][-1][\"content\"] for op in output])\n",
    "pp([op[0][\"generated_text\"] for op in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc3d4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect\\n'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect\\n'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'correct\\n'\u001b[0m,\n",
       "    \u001b[32m'correct\\n'\u001b[0m,\n",
       "    \u001b[32m'correct\\n'\u001b[0m,\n",
       "    \u001b[32m'correct\\n'\u001b[0m,\n",
       "    \u001b[32m'correct\\n'\u001b[0m,\n",
       "    \u001b[32m'incorrect\\n'\u001b[0m,\n",
       "    \u001b[32m'incorrect\\n'\u001b[0m,\n",
       "    \u001b[32m'correct\\n'\u001b[0m,\n",
       "    \u001b[32m'correct\\n'\u001b[0m,\n",
       "    \u001b[32m'correct\\n'\u001b[0m,\n",
       "    \u001b[32m'correct\\n'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp([op[0][\"generated_text\"] for op in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a450ae04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span><span style=\"font-weight: bold\">}]</span>,\n",
       "    <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span><span style=\"font-weight: bold\">}]</span>,\n",
       "    <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span><span style=\"font-weight: bold\">}]</span>,\n",
       "    <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span><span style=\"font-weight: bold\">}]</span>,\n",
       "    <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span><span style=\"font-weight: bold\">}]</span>,\n",
       "    <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect\\n'</span><span style=\"font-weight: bold\">}]</span>,\n",
       "    <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect\\n'</span><span style=\"font-weight: bold\">}]</span>,\n",
       "    <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span><span style=\"font-weight: bold\">}]</span>,\n",
       "    <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span><span style=\"font-weight: bold\">}]</span>,\n",
       "    <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span><span style=\"font-weight: bold\">}]</span>,\n",
       "    <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'correct\\n'</span><span style=\"font-weight: bold\">}]</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'generated_text'\u001b[0m: \u001b[32m'correct\\n'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'generated_text'\u001b[0m: \u001b[32m'correct\\n'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'generated_text'\u001b[0m: \u001b[32m'correct\\n'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'generated_text'\u001b[0m: \u001b[32m'correct\\n'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'generated_text'\u001b[0m: \u001b[32m'correct\\n'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'generated_text'\u001b[0m: \u001b[32m'incorrect\\n'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'generated_text'\u001b[0m: \u001b[32m'incorrect\\n'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'generated_text'\u001b[0m: \u001b[32m'correct\\n'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'generated_text'\u001b[0m: \u001b[32m'correct\\n'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'generated_text'\u001b[0m: \u001b[32m'correct\\n'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'generated_text'\u001b[0m: \u001b[32m'correct\\n'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832ffa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da6a3623a26418eb5df302aa8b5d721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "torch.random.manual_seed(666)\n",
    "\n",
    "with open('./data/betl2train_with_s_a_id.json', 'r') as file_reader:\n",
    "    betl2train = json.load(file_reader)\n",
    "\n",
    "model_op = dict()\n",
    "\n",
    "model_name = \"google/gemma-3-4b-it\"\n",
    "model_op[model_name] = []\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model_name,\n",
    "    device = \"cuda\",\n",
    "    torch_dtype = torch.bfloat16,\n",
    ")\n",
    "\n",
    "m = []\n",
    "for s_a_id in betl2train:\n",
    "\n",
    "    prompt = f'''Question: {betl2train[s_a_id]['q']}\\n'''\n",
    "\n",
    "    r_a = \"\"\n",
    "    for i in range(len(betl2train[s_a_id]['r_a'])):\n",
    "        r_a = r_a + f'''\\nReference Answer {i + 1}: {betl2train[s_a_id]['r_a'][i]}\\n'''\n",
    "\n",
    "    prompt = prompt + r_a + f'''\\nStudent Answer: {betl2train[s_a_id]['s_a']}\\n\\nWhat is the Student's grade?'''\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert grader. You will be provided with a question, a set of reference answers which should be considered the golden standard, and a student's answer. Your task is to grade the student's answer as 'correct' or  'incorrect' based *only* on its semantic alignment with the reference answer. Output *EXACTLY ONLY ONLY ONE SINGLE WORD GRADE* that is either 'correct' or 'incorrect' and *NOTHING ELSE*.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    m.append(messages)\n",
    "\n",
    "output = pipe(m,\n",
    "              do_sample = True,\n",
    "              max_new_tokens = 2,\n",
    "              batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464949d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3941\n"
     ]
    }
   ],
   "source": [
    "print(len(output))\n",
    "\n",
    "model_op[model_name] = [op[0][\"generated_text\"][-1][\"content\"] for op in output]\n",
    "\n",
    "with open('./results/t0_gemma_results.txt', 'w') as f:\n",
    "    for line in model_op[model_name]:\n",
    "        f.write(f\"{str(line).strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d55e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca7b17c61724286a7c5549dc83f8c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import json\n",
    "\n",
    "torch.random.manual_seed(666)\n",
    "\n",
    "with open('./data/betl2train_with_s_a_id.json', 'r') as file_reader:\n",
    "    betl2train = json.load(file_reader)\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model_id,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "m = []\n",
    "\n",
    "for s_a_id in betl2train:\n",
    "\n",
    "    prompt = f'''Question: {betl2train[s_a_id]['q']}\\n'''\n",
    "\n",
    "    r_a = \"\"\n",
    "    for i in range(len(betl2train[s_a_id]['r_a'])):\n",
    "        r_a = r_a + f'''\\nReference Answer {i + 1}: {betl2train[s_a_id]['r_a'][i]}\\n'''\n",
    "\n",
    "    prompt = prompt + r_a + f'''\\nStudent Answer: {betl2train[s_a_id]['s_a']}\\n\\nWhat is the Student's grade?'''\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert grader. You will be provided with a question, a set of reference answers which should be considered the golden standard, and a student's answer. Your task is to grade the student's answer as 'correct' or  'incorrect' based *only* on its semantic alignment with the reference answer. Output *EXACTLY ONLY ONE SINGLE WORD GRADE* that is either 'correct' or 'incorrect' and *NOTHING ELSE*. *NO EXPLANATIONS ARE NECCESARY*\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    m.append(messages)\n",
    "\n",
    "outputs = pipe(\n",
    "    m,\n",
    "    max_new_tokens = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3941\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs))\n",
    "\n",
    "model_op = [op[0][\"generated_text\"][-1]['content'].lower().strip() for op in outputs]\n",
    "\n",
    "with open('./results/t0_llama_results.txt', 'w') as f:\n",
    "    for line in model_op:\n",
    "        f.write(f\"{str(line).strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb7075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309ccdab327b4ab091ceb7e0114daa2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "torch.random.manual_seed(666)\n",
    "\n",
    "with open('./data/betl2train_with_s_a_id.json', 'r') as file_reader:\n",
    "    betl2train = json.load(file_reader)\n",
    "\n",
    "model_op = dict()\n",
    "\n",
    "model_path = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    " \n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 2,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.1,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "m = []\n",
    "for s_a_id in betl2train:\n",
    "\n",
    "    prompt = f'''Question: {betl2train[s_a_id]['q']}\\n'''\n",
    "\n",
    "    r_a = \"\"\n",
    "    for i in range(len(betl2train[s_a_id]['r_a'])):\n",
    "        r_a = r_a + f'''\\nReference Answer {i + 1}: {betl2train[s_a_id]['r_a'][i]}\\n'''\n",
    "\n",
    "    prompt = prompt + r_a + f'''\\nStudent Answer: {betl2train[s_a_id]['s_a']}\\n\\nWhat is the Student's grade?'''\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert grader. You will be provided with a question, a set of reference answers which should be considered the golden standard, and a student's answer. Your task is to grade the student's answer as 'correct' or  'incorrect' based *only* on its semantic alignment with the reference answer. Output *EXACTLY ONLY ONE SINGLE WORD GRADE* that is either 'correct' or 'incorrect' and *NOTHING ELSE*. *NO EXPLANATIONS ARE NECCESARY*\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    m.append(messages)\n",
    "\n",
    "output = pipe(m,\n",
    "              **generation_args,\n",
    "              batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796de0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3941\n"
     ]
    }
   ],
   "source": [
    "print(len(output))\n",
    "\n",
    "model_op[model_path] = [op[0]['generated_text'] for op in output]\n",
    "\n",
    "with open('./results/t0_phi_results.txt', 'w') as f:\n",
    "    for line in model_op[model_path]:\n",
    "        f.write(f\"{str(line).strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fdd589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
