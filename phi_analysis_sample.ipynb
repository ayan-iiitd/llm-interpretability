{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4709fc75",
   "metadata": {},
   "source": [
    "##### token probability per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339369ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import re\n",
    "from rich import print as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d25c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_all_data = \"../data/text_with_entities.json\"\n",
    "\n",
    "with open(path_to_all_data, 'r') as json_file:\n",
    "    ddi_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_probs_per_layer(model, tokenizer, input_text, tokens_of_interest):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Analyze a model's hidden states to track token probabilities layer by layer.\n",
    "\n",
    "    Args:\n",
    "        model: The HuggingFace transformer model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        input_text (str): The prompt to analyze.\n",
    "        tokens_of_interest (list[str]): A list of specific tokens whose probability we want to track.\n",
    "\n",
    "    Returns:\n",
    "        dict: A results dictionary containing:\n",
    "            - A list of dictionaries, one for each layer.\n",
    "            - Each list item has the layer index, probabilities for the tokens of interest and the 5 tokens with the highest probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Tokenize input\n",
    "    inputs = tokenizer(input_text, return_tensors = \"pt\").to(model.device)\n",
    "    \n",
    "    ## Get token IDs for tokens of interest\n",
    "    token_ids_of_interest = []\n",
    "    for token in tokens_of_interest:\n",
    "        token_id = tokenizer.encode(\" \" + token, add_special_tokens = False)[0] ## if word is tokenized into multiple tokens, so using the first one: {token_id[0]}\n",
    "        token_ids_of_interest.append(token_id)\n",
    "    \n",
    "    \n",
    "    generated_ids = model.generate(**inputs,\n",
    "                                   max_new_tokens = 50,\n",
    "                                   pad_token_id = tokenizer.eos_token_id)\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens = True)\n",
    "    \n",
    "    ## Run model with hidden states output for jus the prompt\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states = True)\n",
    "    \n",
    "    ## Get the layer of the model that projects hidden states to vocabulary logits\n",
    "    output_projection = model.get_output_embeddings()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    ## For each layer find top probable tokens and probabilities of tokens of interest\n",
    "    for layer_idx, layer_hidden_state in enumerate(outputs.hidden_states):\n",
    "        \n",
    "        ## Get last token position hidden state\n",
    "        last_token_hidden = layer_hidden_state[0, -1, :]\n",
    "        \n",
    "        ## Project to vocabulary space\n",
    "        logits = output_projection(last_token_hidden)\n",
    "        \n",
    "        ## Convert to probabilities\n",
    "        probs = torch.softmax(logits, dim = -1)\n",
    "        \n",
    "        ## Extract probabilities for tokens of interest\n",
    "        token_probs = {token: probs[token_id].item()\\\n",
    "                       for token, token_id in zip(tokens_of_interest, token_ids_of_interest)}\n",
    "        \n",
    "        top_5_probs, top_5_ids = torch.topk(probs, 5)\n",
    "        top_5_tokens = [\n",
    "            {\n",
    "                \"token\": tokenizer.decode(token_id),\n",
    "                \"probability\": prob.item()\n",
    "            }\n",
    "            for token_id, prob in zip(top_5_ids, top_5_probs)\n",
    "        ]\n",
    "\n",
    "        results.append({\n",
    "            \"layer\": layer_idx,\n",
    "            \"tokens_of_interest\": token_probs,\n",
    "            \"top_5_tokens\": top_5_tokens\n",
    "        })\n",
    "        \n",
    "        results.append((layer_idx, token_probs))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe9e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'[\\x00-\\x1F\\x7F]'\n",
    "model_name = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"auto\",\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "for idx, entry in enumerate(ddi_data):\n",
    "\n",
    "    tokens_to_check = ddi_data[entry]['entities']\n",
    "    eval_prompt = \"The drugs, chemicals and medical entitites mentioned in the text. - \\\"\" + ddi_data[entry]['full_text'] + \"\\\" are the following: \"\n",
    "    eval_prompt = re.sub(pattern, '', eval_prompt)\n",
    "    results = get_token_probs_per_layer(model, tokenizer, eval_prompt, tokens_to_check)\n",
    "\n",
    "    with open(f'../results/token_prob/phi/sample_{idx + 1}.json', 'w') as file_writer:\n",
    "        json.dump(results, file_writer)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17841220",
   "metadata": {},
   "source": [
    "##### attention flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db72d8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ayan\\gitlab_repos\\llm_interpretability_entity_extraction\\conda_env\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e278d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_all_data = \"../data/text_with_entities.json\"\n",
    "\n",
    "with open(path_to_all_data, 'r') as json_file:\n",
    "    ddi_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40cde4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_visualize_attention_rollout(model, tokenizer, input_text, show_plot = False, scale = 'log'):\n",
    "    \"\"\"\n",
    "    Calculate and visualize attention rollout for a model and input text.\n",
    "\n",
    "    Args:\n",
    "        model: The Hugging Face transformer model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        input_text (str): The prompt to analyze.\n",
    "        show_plot (bool): If True, displays the plot. Otherwise, returns fig and ax.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - rollout (torch.Tensor): The final [seq_len, seq_len] attention rollout matrix.\n",
    "            - (fig, ax) (tuple): The matplotlib figure and axis objects for the plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = model.device\n",
    "    inputs = tokenizer(input_text, return_tensors = \"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions = True)\n",
    "    \n",
    "    attention_matrices = outputs.attentions\n",
    "    input_tokens = [tokenizer.decode(token_id) for token_id in inputs['input_ids'][0]]\n",
    "    seq_len = len(input_tokens)\n",
    "\n",
    "    ## Calculate attn rollout\n",
    "    rollout = torch.eye(seq_len, device = device)\n",
    "    for layer_attention in attention_matrices:\n",
    "        \n",
    "        avg_head_attention = layer_attention.squeeze(0).mean(dim = 0)\n",
    "        identity_matrix = torch.eye(seq_len, device = device)\n",
    "        residual_attention = 0.5 * avg_head_attention + 0.5 * identity_matrix\n",
    "        rollout = residual_attention @ rollout\n",
    "\n",
    "    ## Calculate percentage contribution scores\n",
    "    raw_scores = rollout.sum(dim = 0)\n",
    "    probabilistic_scores = raw_scores / raw_scores.sum()\n",
    "    contribution_scores = probabilistic_scores.cpu().numpy()\n",
    "    \n",
    "    ## Create dataFrame to store values\n",
    "    contributions = pd.DataFrame({\n",
    "        'token': input_tokens,\n",
    "        'contribution_score': contribution_scores\n",
    "    })\n",
    "\n",
    "\n",
    "    ## For visualization\n",
    "    if show_plot:\n",
    "        \n",
    "        rollout_data = rollout.cpu().numpy()\n",
    "        cbar_label = \"Attention Score\"\n",
    "        plot_title = f\"Attention Rollout for Prompt:\\n'{input_text}'\"\n",
    "\n",
    "        if scale == 'log':\n",
    "            rollout_data = np.log(rollout_data + 1e-9)\n",
    "            cbar_label = \"Log Attention Score\"\n",
    "            plot_title = f\"Log-Scaled Attention Rollout for Prompt:\\n'{input_text}'\"\n",
    "        \n",
    "        fig1, ax1 = plt.subplots(figsize = (14, 12))\n",
    "        sns.heatmap(rollout_data, annot = False, cmap = 'magma', ax = ax1, cbar_kws = {'label': cbar_label})\n",
    "        ax1.set_xticks(np.arange(seq_len) + 0.5)\n",
    "        ax1.set_xticklabels(input_tokens, rotation=90, ha = \"center\")\n",
    "        ax1.set_yticks(np.arange(seq_len) + 0.5)\n",
    "        ax1.set_yticklabels(input_tokens, rotation=0, va = \"center\")\n",
    "        ax1.set_xlabel(\"Attended To (Source Tokens)\")\n",
    "        ax1.set_ylabel(\"Attention From (Target Representations)\")\n",
    "        ax1.set_title(plot_title, fontsize = 16)\n",
    "        plt.tight_layout(pad = 2.0)\n",
    "        plt.show()\n",
    "\n",
    "        fig2, ax2 = plt.subplots(figsize = (12, 8))\n",
    "        sns.barplot(x = 'token',\n",
    "                    y = 'contribution_score',\n",
    "                    data = contributions,\n",
    "                    palette = 'viridis',\n",
    "                    ax = ax2)\n",
    "        ax2.set_xticklabels(ax2.get_xticklabels(), rotation = 45, ha = \"right\")\n",
    "        ax2.set_title(\"Overall Token Contribution Scores (from Attention Rollout)\")\n",
    "        ax2.set_xlabel(\"Input Token\")\n",
    "        ax2.set_ylabel(\"Total Contribution Score (Column Sum)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c59387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27affa9d49a740b1a59c81088d6291e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top Contributing Tokens ---\n",
      "             token  contribution_score\n",
      "0              The        9.999976e-01\n",
      "1            drugs        1.618518e-07\n",
      "2                ,        8.151707e-07\n",
      "3        chemicals        3.443011e-08\n",
      "4              and        5.354339e-08\n",
      "5          medical        2.770667e-08\n",
      "6              ent        2.488946e-08\n",
      "7               it        2.749413e-08\n",
      "8             ites        3.545462e-08\n",
      "9        mentioned        6.290023e-08\n",
      "10              in        6.653143e-08\n",
      "11             the        3.193286e-07\n",
      "12            text        2.490004e-08\n",
      "13               .        7.352909e-08\n",
      "14               -        2.770193e-08\n",
      "15               \"        3.526010e-08\n",
      "16              In        4.115075e-08\n",
      "17           order        1.195736e-08\n",
      "18              to        2.296895e-08\n",
      "19         provide        1.880360e-08\n",
      "20     information        1.773302e-08\n",
      "21             for        3.023531e-08\n",
      "22             the        1.130675e-07\n",
      "23     appropriate        1.057180e-08\n",
      "24         package        1.458008e-08\n",
      "25          insert        1.611313e-08\n",
      "26        labeling        1.283747e-08\n",
      "27              of        1.665403e-08\n",
      "28             pro        9.777012e-09\n",
      "29            gest        1.053360e-08\n",
      "30              in        1.089652e-08\n",
      "31           -only        8.777788e-09\n",
      "32            oral        7.150645e-09\n",
      "33        contrace        9.013450e-09\n",
      "34              pt        5.276289e-09\n",
      "35            ives        6.478932e-09\n",
      "36               (        9.385540e-09\n",
      "37              PO        2.793292e-09\n",
      "38               C        3.529367e-09\n",
      "39               )        6.356478e-09\n",
      "40              in        7.560026e-09\n",
      "41             the        2.329691e-08\n",
      "42              US        4.571259e-09\n",
      "43               ,        9.726165e-09\n",
      "44               a        4.119359e-09\n",
      "45   comprehensive        2.971491e-09\n",
      "46          review        3.529233e-09\n",
      "47             was        3.637941e-09\n",
      "48            made        2.308210e-09\n",
      "49              of        4.145283e-09\n",
      "50               n        3.220195e-09\n",
      "51             org        3.212739e-09\n",
      "52             est        2.648344e-09\n",
      "53             rel        3.506690e-09\n",
      "54               (        4.116377e-09\n",
      "55               0        1.581139e-09\n",
      "56               .        1.968692e-09\n",
      "57             075        9.865225e-10\n",
      "58              mg        1.470257e-09\n",
      "59               )        1.525907e-09\n",
      "60             and        1.790749e-09\n",
      "61            nore        8.306008e-10\n",
      "62              th        5.722214e-10\n",
      "63             ind        5.883462e-10\n",
      "64            rone        8.494979e-10\n",
      "65               (        1.064458e-09\n",
      "66               0        6.331625e-10\n",
      "67               .        5.671239e-10\n",
      "68              35        3.264537e-10\n",
      "69              mg        3.522108e-10\n",
      "70              ),        9.416257e-10\n",
      "71            with        7.842776e-10\n",
      "72             the        1.185081e-09\n",
      "73        clinical        5.179930e-10\n",
      "74     differences        5.995920e-10\n",
      "75       indicated        3.741136e-10\n",
      "76           where        3.815770e-10\n",
      "77      applicable        2.311681e-10\n",
      "78               .        8.112284e-10\n",
      "79                        3.014154e-10\n",
      "80               \"        4.400517e-10\n",
      "81             are        2.897202e-10\n",
      "82             the        9.423529e-11\n",
      "83       following        6.667646e-11\n",
      "84               :        6.071622e-11\n",
      "85                        1.873345e-11\n"
     ]
    }
   ],
   "source": [
    "pattern = r'[\\x00-\\x1F\\x7F]'\n",
    "model_name = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"auto\",\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "for idx, entry in enumerate(ddi_data):\n",
    "\n",
    "    tokens_to_check = ddi_data[entry]['entities']\n",
    "    eval_prompt = \"The drugs, chemicals and medical entitites mentioned in the text. - \\\"\" + ddi_data[entry]['full_text'] + \"\\\" are the following: \"\n",
    "    eval_prompt = re.sub(pattern, '', eval_prompt)\n",
    "\n",
    "    contribution_df = calculate_and_visualize_attention_rollout(model,\n",
    "                                                                tokenizer,\n",
    "                                                                eval_prompt,\n",
    "                                                                scale = 'log')\n",
    "    \n",
    "    print(\"\\n--- Top Contributing Tokens ---\")\n",
    "    print(contribution_df.to_string())\n",
    "\n",
    "    contribution_df.to_csv(f'../results/attn_rollout/phi/sample_{idx + 1}.csv', index = False)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da82ac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_rollout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfinal_rollout\u001b[49m.shape\n",
      "\u001b[31mNameError\u001b[39m: name 'final_rollout' is not defined"
     ]
    }
   ],
   "source": [
    "final_rollout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ef29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>contribution_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>8.600391e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>7.010803e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the</td>\n",
       "      <td>2.746357e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drugs</td>\n",
       "      <td>1.391992e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>the</td>\n",
       "      <td>9.724270e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>applicable</td>\n",
       "      <td>1.988141e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>the</td>\n",
       "      <td>8.104623e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>following</td>\n",
       "      <td>5.734450e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>:</td>\n",
       "      <td>5.221844e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td></td>\n",
       "      <td>1.611154e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token  contribution_score\n",
       "0           The        8.600391e+01\n",
       "2             ,        7.010803e-05\n",
       "11          the        2.746357e-05\n",
       "1         drugs        1.391992e-05\n",
       "22          the        9.724270e-06\n",
       "..          ...                 ...\n",
       "77   applicable        1.988141e-08\n",
       "82          the        8.104623e-09\n",
       "83    following        5.734450e-09\n",
       "84            :        5.221844e-09\n",
       "85                     1.611154e-09\n",
       "\n",
       "[86 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee35790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
